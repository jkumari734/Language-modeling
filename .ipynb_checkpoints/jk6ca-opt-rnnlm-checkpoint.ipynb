{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_read(filename):\n",
    "    f = open(filename, \"r\",encoding=\"utf8\")\n",
    "    sentences = []\n",
    "\n",
    "    for x in f:\n",
    "        sentences.append(x.split())\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tags(sentences):\n",
    "    tags = {}\n",
    "    \n",
    "    c = 0\n",
    "    for i in sentences:\n",
    "        for j in i:\n",
    "            if j not in tags:\n",
    "                tags[j] = c\n",
    "                c += 1\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(tags, sentences):\n",
    "    training_data = []\n",
    "    \n",
    "    for i in sentences:\n",
    "        temp = []\n",
    "        for j in i:\n",
    "#             x = tags[j]\n",
    "            temp.append(tags[j])\n",
    "        \n",
    "        training_data.append(temp)\n",
    "        \n",
    "    return training_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = file_read('trn-wiki.txt')\n",
    "sentences_dev = file_read('dev-wiki.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = generate_tags(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = data_preprocess(tags, sentences)\n",
    "data_dev = data_preprocess(tags, sentences_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 32\n",
    "HIDDEN_DIM = 32\n",
    "\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(tags), len(tags))\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize = [optim.Adadelta(model.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0), \n",
    "            optim.Adagrad(model.parameters(), lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10), \n",
    "            optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False), \n",
    "            optim.Adamax(model.parameters(), lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0), \n",
    "            optim.ASGD(model.parameters(), lr=0.01, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0), \n",
    "#             optim.LBFGS(model.parameters(), lr=1, max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, history_size=100, line_search_fn=None), \n",
    "            optim.SGD(model.parameters(), lr=0.1, momentum=0, dampening=0, weight_decay=0, nesterov=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(data, model):\n",
    "    s = 0\n",
    "    gt_length = 0\n",
    "    for i in data:\n",
    "        with torch.no_grad():\n",
    "            inputs = i[:-1]\n",
    "            inputs = torch.LongTensor(inputs)\n",
    "            tag_scores = model(inputs) \n",
    "            gt = i[1:]\n",
    "            gt_length += len(gt)\n",
    "            x = 0\n",
    "            for j in gt:\n",
    "                s += math.log(tag_scores[x][j])      #check for log math domain error\n",
    "            x += 1\n",
    "                \n",
    "        break\n",
    "    perplex = math.exp(-(s/gt_length))\n",
    "    \n",
    "    return perplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, optimize, model, data):\n",
    "    \n",
    "    for j in optimize: \n",
    "        optimizer = j\n",
    "\n",
    "        for epoch in range(epochs): \n",
    "            for sentence in training_data:\n",
    "                # Step 1. Remember that Pytorch accumulates gradients.\n",
    "                # We need to clear them out before each instance\n",
    "                model.zero_grad()\n",
    "                sentence = torch.LongTensor(sentence)\n",
    "\n",
    "                tag_scores = model(sentence[:-1])\n",
    "\n",
    "                loss = loss_function(tag_scores, sentence[1:])\n",
    "                loss.backward()\n",
    "                optimizer.step() \n",
    "                break\n",
    "\n",
    "        print(perplexity(training_data, model),\"training accuracy\")\n",
    "        print(perplexity(data_dev, model),\"dev accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28556.141082440354 training accuracy\n",
      "27794.003889381263 dev accuracy\n",
      "27872.878364831948 training accuracy\n",
      "27495.297208335443 dev accuracy\n",
      "27836.79424320967 training accuracy\n",
      "27479.33105876747 dev accuracy\n",
      "27765.40843270224 training accuracy\n",
      "27447.747402747387 dev accuracy\n",
      "27765.406047930304 training accuracy\n",
      "27447.746145590554 dev accuracy\n",
      "27765.400391370276 training accuracy\n",
      "27447.741501015724 dev accuracy\n"
     ]
    }
   ],
   "source": [
    "train(1, optimize, model, training_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
